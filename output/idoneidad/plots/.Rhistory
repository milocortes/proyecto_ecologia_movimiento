times <- seq(init,final,time_step)
# Método de integración
intg.method<-c("rk4")
GenExp <- function(x, x_val, y_val, s, min){
return((y_val - min) * exp((s/(y_val-min))*(x-x_val)) + min)
}
# Definimos el modelo a resolver
model <- function(t, state, parameters) {
with(as.list(c(state,parameters)), {
#auxiliary endogenous variables
#flow variables
L = Y/a
K = Y/nu
Pi = Y * (w*L) - (r*D)
lambda = L/N
PC = GenExp(lambda, 0.95, 0, 0.5, -0.01)
I = GenExp(Pi/(ν*Y),0.05,0.05,1.75,0)
#state variables
dY = ((I/nu) * gamma) * Y
dw = PC * w
dD = I - Pi
da = alpha * a
dN = beta * N
list(c(dY,dw,dD,da,dN),Pi= Pi, L = L, K = K)
})
}
# Simulate model
out <- ode(y = InitialConditions, # y recibe las condiciones iniciales
times = times, # times recibe el arreglo del intervalo de tiempo
func = model, # func recibe el modelo a resolver
parms = parameters,  # parms recibe los parámetros del modelo
method = intg.method  # method recibe el método de integración
)
# Limpiamos ambiente de trabajo
rm(list=ls())
# Cargamos bibliotecas
library(deSolve)
# Arreglo de parámetros
parameters<-c(beta = 0.01 , alpha = 0.02 , d = 5, c = 4.8 , gamma = 0.01, nu = 3, r = 0.05)
# Arreglo de condiciones iniciales
InitialConditions <- c(Y = 300,w = 0.95 ,D = 0, a = 1, N = 300)
# Arreglo del intervalo de tiempo
init <- 0
final <- 100
time_step <- 1/4
times <- seq(init,final,time_step)
# Método de integración
intg.method<-c("rk4")
GenExp <- function(x, x_val, y_val, s, min){
return((y_val - min) * exp((s/(y_val-min))*(x-x_val)) + min)
}
# Definimos el modelo a resolver
model <- function(t, state, parameters) {
with(as.list(c(state,parameters)), {
#auxiliary endogenous variables
#flow variables
L = Y/a
K = Y/nu
Pi = Y * (w*L) - (r*D)
lambda = L/N
PC = GenExp(lambda, 0.95, 0, 0.5, -0.01)
I = GenExp(Pi/(nu*Y),0.05,0.05,1.75,0)
#state variables
dY = ((I/nu) * gamma) * Y
dw = PC * w
dD = I - Pi
da = alpha * a
dN = beta * N
list(c(dY,dw,dD,da,dN),Pi= Pi, L = L, K = K)
})
}
# Simulate model
out <- ode(y = InitialConditions, # y recibe las condiciones iniciales
times = times, # times recibe el arreglo del intervalo de tiempo
func = model, # func recibe el modelo a resolver
parms = parameters,  # parms recibe los parámetros del modelo
method = intg.method  # method recibe el método de integración
)
out
# Limpiamos ambiente de trabajo
rm(list=ls())
# Cargamos bibliotecas
library(deSolve)
# Arreglo de parámetros
parameters<-c(α = 0.02,β = 0.01,c = 4.8,d = 5,γ = 0.01,ν = 3,r = 0.05)
# Arreglo de condiciones iniciales
InitialConditions <- c(Y =300,
D = 0,
w = 0.95,
a = 1,
N = 300)
# Arreglo del intervalo de tiempo
init <- 0
final <- 100
time_step <- 1/4
times <- seq(init,final,time_step)
# Método de integración
intg.method<-c("rk4")
# Definimos la función no lineal
GenExp <- function(x,x_val,y_val,s,min){
return ( (y_val - min) * exp( (s/(y_val - min))*(x - x_val)) + min)
}
# Definimos el modelo a resolver
model <- function(t, state, parameters) {
with(as.list(c(state,parameters)), {
#flow variables
L = Y/a
K = (1/ν) * Y
ganancias  = Y - w*L - r*D
λ = L/N
PC = GenExp(λ,0.95,0,0.5,-0.01)
I = GenExp(ganancias/(ν*Y),0.05,0.05,1.75,0)
# state variables
dY = ((I/ν) - γ ) * Y
dD = I * Y - ganancias
dw = PC*w
da = α*a
dN = β*N
list(c(dY,dD,dw,da,dN),ganancias = ganancias,L = L,K = K)
})
}
# Simulate model
out <- ode(y = InitialConditions, # y recibe las condiciones iniciales
times = times, # times recibe el arreglo del intervalo de tiempo
func = model, # func recibe el modelo a resolver
parms = parameters,  # parms recibe los parámetros del modelo
method = intg.method  # method recibe el método de integración
)
out
plot(out)
# Visualizamos los resultados
out <- data.frame(out)
out$wage_share <- (out$w* out$L) / out$Y
out$employment_rate <- out$L  / out$N
out$debt_to_output_ratio <- out$D / out$Y
install.packages("plotly")
#install.packages("plotly")
library(plotly)
fig <- plot_ly(out, x = ~wage_share, y = ~employment_rate, z = ~debt_to_output_ratio, type = 'scatter3d', mode = 'lines',
line = list(width = 4, color = ~time, colorscale = list(c(0,'#BA52ED'), c(1,'#FCB040')) , showscale = TRUE))
fig <- fig %>% layout(scene = list(xaxis = list(title = 'Wage share'),
yaxis = list(title = 'Employment rate'),
zaxis = list(title = 'Debt to output')))
fig
q()
rm(list=ls())
# Load libraries
library(amt)
library(tidyverse)
library (rgdal)
library(raster)
# Set credential
loginStored <- movebankLogin(username="", password="")
gallopavo_all <- getMovebankData(study="Eastern Wild Turkey_Margadant", login=loginStored)
gallopavo_all
# Get track GPS data
gallopavo <- getMovebankLocationData(study="Eastern Wild Turkey_Margadant" , sensorID="GPS", login=loginStored,removeDuplicatedTimestamps=TRUE)
# Remove NAN
gallopavo <- gallopavo %>% drop_na()
# Set wd
setwd("/home/milo/PCIC/Maestría/4toSemestre/ecol_mov/proyecto/data/raster")
################ Indice de humedad
wetness <- raster("wetness_gallopavo.tif")
names(wetness) <- "wetness_"
################ Elevacion
elevation <- raster("elv_gallopavo.tif")
names(elevation) <- "elevation_"
################# Temperatura
temperature <- raster("lst_gallopavo.tif")
names(temperature) <- "temperature_"
################# EVI
evi <- raster("evi_gallopavo.tif")
names(evi) <- "evi_"
# 4. Ahora abrimos los datos de movimiento de una manera un poco diferente a usando tidyverse.
dat <- gallopavo %>%
mutate(x = `location.long`, y = `location.lat`,
t = as.POSIXct(gallopavo$timestamp, format= "%Y-%m-%d %H:%M:S", tz="UTC"), id = `individual.local.identifier`)
# 4.1 Exploramos lo datos con un solo indiviudo
gallo_uno <- subset(dat, individual.local.identifier == "Sky_462")
# 4.2 Creamos el objeto track que puede leer amt
tr1 <- make_track(gallo_uno, .x = x, .y=y, .t=t,.id =id,crs = 4326)
tr1 <- transform_coords(tr1,crs_to = 32616)
# Generate random spatial points
# KDE
# Usamos un un objeto SpatialPolygons*, el cual puede contener poligonos o mulipoligonos.
# Esto puede ser bastante util cuando necesitamos adehir un area adicional a las area de actividad para definir de mejor
# manera lo que podria estar disponible para los animales en las areas que estan utilizando. Al igual que el ejemplo anterior
# con este metodo tambien se puede utilizar el argumento "presence".
hr_4 <- hr_kde(tr1, level = 0.95) %>% hr_isopleths() %>%
sf::st_buffer(dist =3000) # incluimos un buffer de 3 km
r6 <- random_points(hr_4, n = 45190, presence = tr1)
# 6. Ahora el siguiente paso antes de ajustar el RSF es extraer el valor de las covariables (rasters que est�n arriba)
# tanto para las localizaciones como para los puntos al azar. Con el comando extract_covariates extraemos el valor de
# la celda donde est� cada una de las localizaciones.
rsf1 <- r6 %>%
extract_covariates(wetness, where = "end") %>%
extract_covariates(elevation, where = "end") %>%
extract_covariates(temperature, where = "end") %>%
extract_covariates(evi, where = "end")
rsf1
# 7.1 Primero ajustamos el modelo m�s complejo y posteriormente vamos simplificando el modelo con base en la
# logverosimilitud que explique cada una de las covariables
model_1 <- rsf1 %>% fit_rsf(case_ ~ wetness_ + elevation_)
summary(model_1)
AIC(model_1$model)
# *******************************************
# *******************************************
## Modelos de selección de recursos
# *******************************************
# *******************************************
# 4.1 Creamos el objeto track que puede leer amt. Este caso como vamos a trabajar a nivel poblacional al momento de crear el
# objeto track vamos indicarle en el argumento "id" que la columna "treatement" es la que da la distinci�n de individuos,
# pero en este caso todos los elefantes son residentes (not translocated = nt), por lo que el objeto track que crearemos a
# continuac�n ser� como si fuera un solo individuo. Esto nos permitir� crear un pol�gono m�nimo convexo para todos los
# individuos y posteriomente estimar los puntos al azar para evaluar la disponibilidad a nivel poblacional.
gallopavo$treatment <- "nt_gallo"
dat <- gallopavo %>%
mutate(x = `location.long`, y = `location.lat`,
t = as.POSIXct(gallopavo$timestamp, format= "%Y-%m-%d %H:%M:S", tz="UTC"), id = `treatment`)
tr1 <- make_track(dat, .x = x, .y=y, .t=t,.id =id,crs = 4326)
tr1 <- transform_coords(tr1,crs_to = 32616)
# 4.1 El siguiente paso es extraer el 10% de las localizaciones para que podamos validar con el m�todo de validaci�n cruzada
# Esto lo hacemos con el siguiente c�digo donde tomamos el 10% de las localizaciones al azar y las llamamos "entrenamiento"
# (training) y al resto localizaciones de prueba (testing).
training <- tr1 %>%
sample_frac(0.1, replace = F)
# Estas localizaciones las utilizaremos posteriomente para la validaci�n. Ahora generamos otro objeto con la localizaciones
# de prueba con las cuales haremos los modelos
testing <- anti_join(tr1, training)
######
# 5. Antes de ajustar un modelo de RSF es necesario preparar los datos. Lo primero que tenemos que hacer es generar datos al
# azar. En este caso estos puntos representan los sitios donde el animal podria haber usado. Los puntos al azar definen lo
# disponible para el animal. En este caso utilizaremos  un objeto SpatialPolygons*, el cual puede contener poligonos o mulipoligonos.
# Esto puede ser bastante util cuando necesitamos adehir un �rea adicional a las �rea de actividad para definir de mejor
# manera lo que podria estar disponible para los animales en las �reas que est�n utilizando. En este caso al Pol�gono M�nimo
# Convexo de los datos de los 4 elefantes vamos adherir un �rea adicional de 5 kilom�tros:
hr_1 <- hr_mcp(testing, level = 1) %>% hr_isopleths() %>%
sf::st_buffer(dist =5000) # incluimos un buffer de 5 km
# Ahora generamos puntos al azar 10 veces m�s de los puntos de las localizaciones:
r1 <- random_points(hr_1, n = 190210, presence = testing)
# 6. Ahora el siguiente paso antes de ajustar el RSF es extraer el valor de las covariables (rasters que est�n arriba)
# tanto para las localizaciones como para los puntos al azar. Con el comando extract_covariates extraemos el valor de
# la celda donde est� cada una de las localizaciones.
rsf1 <- r1 %>%
extract_covariates(wetness, where = "end") %>%
extract_covariates(elevation, where = "end") %>%
extract_covariates(temperature, where = "end") %>%
extract_covariates(evi, where = "end")
# 7. Ya tenemos ahora todas las partes para ajustar el RSF. Para esto vamos a utilizar el comando "fit_rsf" el cual es
# basicamente una adpataci�n de  stats::glm withfamily = binomial(link = "logit").
# En este segundo modelo omito forest_ e incluyo las otras covariables que nos estaban en el otro modelo
model_1 <- rsf1 %>% fit_rsf(case_ ~ wetness_)
model_2 <- rsf1 %>% fit_rsf(case_ ~ elevation_)
model_3 <- rsf1 %>% fit_rsf(case_ ~ temperature_)
model_4 <- rsf1 %>% fit_rsf(case_ ~ evi_)
model_5 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_)
model_6 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ temperature_)
model_7 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ evi_)
model_8 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ temperature_)
model_9 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ evi_)
model_10 <- rsf1 %>% fit_rsf(case_ ~ temperature_+ evi_)
model_11 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ temperature_)
model_12 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ evi_)
model_13 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ temperature_+ evi_)
model_14 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ temperature_+ evi_)
model_15 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ temperature_+ evi_)
library(MuMIn)
# Ahora promedio los modelos:
models1<- model.sel (model_1$model,model_2$model,model_3$model,model_4$model,model_5$model,
model_6$model,model_7$model,model_8$model,model_9$model,model_10$model,model_11$model,model_12$model,model_13$model,model_14$model,model_15$model)
models1
rm(list=ls())
rm(list=ls())
library(move)
library(tidyr)
loginStored <- movebankLogin(username="milocg", password="%Move123")
gallopavo_all <- getMovebankData(study="Eastern Wild Turkey_Margadant", login=loginStored)
gallopavo_all
# Get track GPS data
gallopavo <- getMovebankLocationData(study="Eastern Wild Turkey_Margadant" , sensorID="GPS", login=loginStored,removeDuplicatedTimestamps=TRUE)
# Remove NAN
gallopavo <- gallopavo %>% drop_na()
# Set wd
setwd("/home/milo/PCIC/Maestría/4toSemestre/ecol_mov/proyecto/data/raster")
################ Indice de humedad
wetness <- raster("wetness_gallopavo.tif")
names(wetness) <- "wetness_"
################ Elevacion
elevation <- raster("elv_gallopavo.tif")
names(elevation) <- "elevation_"
################# Temperatura
temperature <- raster("lst_gallopavo.tif")
names(temperature) <- "temperature_"
################# EVI
evi <- raster("evi_gallopavo.tif")
names(evi) <- "evi_"
# 4. Ahora abrimos los datos de movimiento de una manera un poco diferente a usando tidyverse.
dat <- gallopavo %>%
mutate(x = `location.long`, y = `location.lat`,
t = as.POSIXct(gallopavo$timestamp, format= "%Y-%m-%d %H:%M:S", tz="UTC"), id = `individual.local.identifier`)
# 4.1 Exploramos lo datos con un solo indiviudo
gallo_uno <- subset(dat, individual.local.identifier == "Sky_462")
# 4.2 Creamos el objeto track que puede leer amt
tr1 <- make_track(gallo_uno, .x = x, .y=y, .t=t,.id =id,crs = 4326)
tr1 <- transform_coords(tr1,crs_to = 32616)
# Generate random spatial points
# KDE
# Usamos un un objeto SpatialPolygons*, el cual puede contener poligonos o mulipoligonos.
# Esto puede ser bastante util cuando necesitamos adehir un area adicional a las area de actividad para definir de mejor
# manera lo que podria estar disponible para los animales en las areas que estan utilizando. Al igual que el ejemplo anterior
# con este metodo tambien se puede utilizar el argumento "presence".
hr_4 <- hr_kde(tr1, level = 0.95) %>% hr_isopleths() %>%
sf::st_buffer(dist =3000) # incluimos un buffer de 3 km
r6 <- random_points(hr_4, n = 45190, presence = tr1)
# 6. Ahora el siguiente paso antes de ajustar el RSF es extraer el valor de las covariables (rasters que est�n arriba)
# tanto para las localizaciones como para los puntos al azar. Con el comando extract_covariates extraemos el valor de
# la celda donde est� cada una de las localizaciones.
rsf1 <- r6 %>%
extract_covariates(wetness, where = "end") %>%
extract_covariates(elevation, where = "end") %>%
extract_covariates(temperature, where = "end") %>%
extract_covariates(evi, where = "end")
rsf1
# 7.1 Primero ajustamos el modelo m�s complejo y posteriormente vamos simplificando el modelo con base en la
# logverosimilitud que explique cada una de las covariables
model_1 <- rsf1 %>% fit_rsf(case_ ~ wetness_ + elevation_)
summary(model_1)
AIC(model_1$model)
# *******************************************
# *******************************************
## Modelos de selección de recursos
# *******************************************
# *******************************************
# 4.1 Creamos el objeto track que puede leer amt. Este caso como vamos a trabajar a nivel poblacional al momento de crear el
# objeto track vamos indicarle en el argumento "id" que la columna "treatement" es la que da la distinci�n de individuos,
# pero en este caso todos los elefantes son residentes (not translocated = nt), por lo que el objeto track que crearemos a
# continuac�n ser� como si fuera un solo individuo. Esto nos permitir� crear un pol�gono m�nimo convexo para todos los
# individuos y posteriomente estimar los puntos al azar para evaluar la disponibilidad a nivel poblacional.
gallopavo$treatment <- "nt_gallo"
dat <- gallopavo %>%
mutate(x = `location.long`, y = `location.lat`,
t = as.POSIXct(gallopavo$timestamp, format= "%Y-%m-%d %H:%M:S", tz="UTC"), id = `treatment`)
tr1 <- make_track(dat, .x = x, .y=y, .t=t,.id =id,crs = 4326)
tr1 <- transform_coords(tr1,crs_to = 32616)
# 4.1 El siguiente paso es extraer el 10% de las localizaciones para que podamos validar con el m�todo de validaci�n cruzada
# Esto lo hacemos con el siguiente c�digo donde tomamos el 10% de las localizaciones al azar y las llamamos "entrenamiento"
# (training) y al resto localizaciones de prueba (testing).
training <- tr1 %>%
sample_frac(0.1, replace = F)
# Estas localizaciones las utilizaremos posteriomente para la validaci�n. Ahora generamos otro objeto con la localizaciones
# de prueba con las cuales haremos los modelos
testing <- anti_join(tr1, training)
######
# 5. Antes de ajustar un modelo de RSF es necesario preparar los datos. Lo primero que tenemos que hacer es generar datos al
# azar. En este caso estos puntos representan los sitios donde el animal podria haber usado. Los puntos al azar definen lo
# disponible para el animal. En este caso utilizaremos  un objeto SpatialPolygons*, el cual puede contener poligonos o mulipoligonos.
# Esto puede ser bastante util cuando necesitamos adehir un �rea adicional a las �rea de actividad para definir de mejor
# manera lo que podria estar disponible para los animales en las �reas que est�n utilizando. En este caso al Pol�gono M�nimo
# Convexo de los datos de los 4 elefantes vamos adherir un �rea adicional de 5 kilom�tros:
hr_1 <- hr_mcp(testing, level = 1) %>% hr_isopleths() %>%
sf::st_buffer(dist =5000) # incluimos un buffer de 5 km
# Ahora generamos puntos al azar 10 veces m�s de los puntos de las localizaciones:
r1 <- random_points(hr_1, n = 190210, presence = testing)
# 6. Ahora el siguiente paso antes de ajustar el RSF es extraer el valor de las covariables (rasters que est�n arriba)
# tanto para las localizaciones como para los puntos al azar. Con el comando extract_covariates extraemos el valor de
# la celda donde est� cada una de las localizaciones.
rsf1 <- r1 %>%
extract_covariates(wetness, where = "end") %>%
extract_covariates(elevation, where = "end") %>%
extract_covariates(temperature, where = "end") %>%
extract_covariates(evi, where = "end")
# 7. Ya tenemos ahora todas las partes para ajustar el RSF. Para esto vamos a utilizar el comando "fit_rsf" el cual es
# basicamente una adpataci�n de  stats::glm withfamily = binomial(link = "logit").
# En este segundo modelo omito forest_ e incluyo las otras covariables que nos estaban en el otro modelo
model_1 <- rsf1 %>% fit_rsf(case_ ~ wetness_)
model_2 <- rsf1 %>% fit_rsf(case_ ~ elevation_)
model_3 <- rsf1 %>% fit_rsf(case_ ~ temperature_)
model_4 <- rsf1 %>% fit_rsf(case_ ~ evi_)
model_5 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_)
model_6 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ temperature_)
model_7 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ evi_)
model_8 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ temperature_)
model_9 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ evi_)
model_10 <- rsf1 %>% fit_rsf(case_ ~ temperature_+ evi_)
model_11 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ temperature_)
model_12 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ evi_)
model_13 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ temperature_+ evi_)
model_14 <- rsf1 %>% fit_rsf(case_ ~ elevation_+ temperature_+ evi_)
model_15 <- rsf1 %>% fit_rsf(case_ ~ wetness_+ elevation_+ temperature_+ evi_)
library(MuMIn)
# Ahora promedio los modelos:
models1<- model.sel (model_1$model,model_2$model,model_3$model,model_4$model,model_5$model,
model_6$model,model_7$model,model_8$model,model_9$model,model_10$model,model_11$model,model_12$model,model_13$model,model_14$model,model_15$model)
models1
model_15
summary(model_15)
model_15$model
wetness*14.239908
(wetness*14.239908) + (elevation*-0.002116)
(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726)
model_15$model
(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)
29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)
exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920))
exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)) / (1 + exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)))
RSF_raster <- exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)) / (1 + exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)))
RSF_raster
plot(RSF_raster)
RSF_raster <- exp((wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)) / (1 + exp((wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)))
RSF_raster
# Reclasificamos en 10 bins
RSF_bins <- reclassify(RSF_raster, c(0.0,0.10,1,
0.1,0.2,2,
0.2,0.3,3,
0.3,0.4,4,
0.4,0.5,6,
0.5,0.6,6,
0.6,0.7,7,
0.7,0.8,8,
0.8,0.9,9
0.9,1.0,10))
# Reclasificamos en 10 bins
RSF_bins <- reclassify(RSF_raster, c(0.0,0.10,1,
0.1,0.2,2,
0.2,0.3,3,
0.3,0.4,4,
0.4,0.5,6,
0.5,0.6,6,
0.6,0.7,7,
0.7,0.8,8,
0.8,0.9,9,
0.9,1.0,10))
plot(RSF_bins)
plot(RSF_raster)
RSF_raster <- exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)) / (1 + exp(29.210273+(wetness*14.239908) + (elevation*-0.002116) + (temperature*-1.798726) + (evi*35.179920)))
plot(RSF_raster)
RSF_bins <- reclassify(RSF_raster, c(0.0,0.10,1,
0.1,0.2,2,
0.2,0.3,3,
0.3,0.4,4,
0.4,0.5,6,
0.5,0.6,6,
0.6,0.7,7,
0.7,0.8,8,
0.8,0.9,9,
0.9,1.0,10))
plot(RSF_bins)
validacion <- validacion %>%
extract_covariates(RSF_bins, where = "end")
validacion
# Retomamos primero el objeto con los datos de entrenamiento
validacion <- training
validacion <- validacion %>%
extract_covariates(RSF_bins, where = "end")
validacion
summary(validacion)
# Ahora convertimoslos valores obtenidos a factor y los convertimos en una tabla.
val_3<-data.frame(validacion)
###
val_3$val_1 <- as.factor(val_3$val_1)
table(val_3$val_1)
attach(val_3)
summary(val_3)
val_3<-data.frame(validacion)
val_3
head(val_3)
val_3$val_1 <- as.factor(val_3$layer)
table(val_3$val_1)
attach(val_3)
summary(val_3)
validacion
plot(RSF_bins)
table(RSF_bins)
cellStats(RSF_bins, 'sum')
plot(RSF_bins)
plot(RSF_bins==1)
for (i in c(1:10)) {
print(paste0(i,",",cellStats(RSF_bins==i, 'sum')))
}
n_cell_each_bin <-vector()
for (i in c(1:10)) {
n_cell_each_bin[i] <- cellStats(RSF_bins==i, 'sum')
}
n_cell_each_bin
setwd("/home/milo/PCIC/Maestría/4toSemestre/ecol_mov/proyecto/output/idoneidad/raster")
print("Guardamos el raster de idoneidad de habitat")
writeRaster(RSF_raster, filename="idoneidad_habitat.tif", overwrite=TRUE)
RSF_raster
plot(RSF_raster)
plot(RSF_raster)
plot(RSF_raster)
title("Mapa de idoneidad de hábitat")
setwd("/home/milo/PCIC/Maestría/4toSemestre/ecol_mov/proyecto/output/idoneidad/plots")
cairo_ps(file = "idoneidad_habitat.eps"),onefile = FALSE,fallback_resolution=600)
plot(RSF_raster)
title("Mapa de idoneidad de hábitat")
dev.off()
setwd("/home/milo/PCIC/Maestría/4toSemestre/ecol_mov/proyecto/output/idoneidad/plots")
cairo_ps(file = "idoneidad_habitat.eps",onefile = FALSE,fallback_resolution=600)
plot(RSF_raster)
title("Mapa de idoneidad de hábitat")
dev.off()
models1
install.packages("stargazer")
library(stargazer)
stargazer(models1)
stargazer(models1, summary=FALSE, rownames=FALSE)
models1
stargazer(models1, summary=FALSE)
models1
rm(list=ls())
q()
